{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full_stacker_pipeline_crop_kev.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_G8aermNnoC"
      },
      "source": [
        "# Algorithmic methods imports\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage, misc\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM3AbQibrOt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1d83de-0794-4408-f53c-7582a765421e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BQQQybIZIwG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from IPython.display import clear_output "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg1WvW0KOFwj"
      },
      "source": [
        "#data_path = \"./drive/MyDrive/CSC413 Project/trainA\"\n",
        "#label_path = \"./drive/MyDrive/CSC413 Project/trainB\"\n",
        "train_path = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/trainA\"\n",
        "train_label_path = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/trainB\"\n",
        "train_path_cropped = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/trainA/original_cropped\"\n",
        "train_label_path_cropped = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/trainB/original_cropped\"\n",
        "test_path = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/testA\"\n",
        "test_label_path = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/testB\"\n",
        "output_path = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/trainA/autoencoder\"\n",
        "test_output_path = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/testA/autoencoder\"\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLNG3wX9-sAU"
      },
      "source": [
        "# Data cleaning and pre-processing\n",
        "Written by Kevin Zhang 2021-04-08"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HtlaPM7-q36",
        "outputId": "ffc4e7e7-b3c4-47b3-b610-2868b02d1d91"
      },
      "source": [
        "if not os.path.exists(train_path+\"/original_cropped\"):\n",
        "  os.mkdir(train_path+\"/original_cropped\")\n",
        "\n",
        "idx = 0 \n",
        "for img in sorted(os.listdir(train_path)):\n",
        "  if img.endswith('.png'):\n",
        "    input_image_path = os.path.join(train_path,img)\n",
        "    raw_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    output = raw_image[0:256, 0:256].copy()\n",
        " \n",
        "    idx += 1\n",
        "    clear_output()\n",
        "    print(\"processing\", idx)\n",
        "    cv2.imwrite(os.path.join(train_path+\"/original_cropped\", img), output)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing 2448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bI5updTINTq",
        "outputId": "b2654fa4-0170-4fb9-aad4-4313c94d6e83"
      },
      "source": [
        "if not os.path.exists(train_label_path+\"/original_cropped\"):\n",
        "  os.mkdir(train_label_path+\"/original_cropped\")\n",
        " \n",
        "idx = 0\n",
        "for img in sorted(os.listdir(train_label_path)):\n",
        "  if img.endswith('.png'):\n",
        "    input_image_path = os.path.join(train_label_path,img)\n",
        "    raw_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    output = raw_image[0:256, 0:256].copy()\n",
        " \n",
        "    idx += 1\n",
        "    clear_output()\n",
        "    print(\"processing\", idx)\n",
        "    cv2.imwrite(os.path.join(train_label_path+\"/original_cropped\", img), output)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing 2448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAZenmvaULBG"
      },
      "source": [
        "# Defining algorithmic processing methods\n",
        "Written by Kevin Zhang"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1y2qC3sT4zq"
      },
      "source": [
        "########################################################\n",
        "# PART 1 - Median Filtering\n",
        "########################################################\n",
        "def median_filter(image_path):\n",
        "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) # Load image\n",
        "  img_median = cv2.medianBlur(img, 25) # Add median filter to image\n",
        "  result = np.minimum(img.astype(np.uint16)+(255-img_median.astype(np.uint16)), 255)\n",
        "  return result.astype(np.uint8)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSEalB-aUGjg"
      },
      "source": [
        "########################################################\n",
        "# PART 2 - Edge Detection, Dilation Erosion\n",
        "########################################################\n",
        "def edge_dilation_erosion_filter(image_path):\n",
        "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) # Load image\n",
        "  edges = cv2.Canny(img,100,200)\n",
        "  dilated = cv2.dilate(edges, np.ones((3,3),np.uint8), iterations=1)\n",
        "  eroded = cv2.erode(dilated, np.ones((4,4),np.uint8), iterations=1)\n",
        "  return cv2.bitwise_not(eroded)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0JF0WqEUHlV"
      },
      "source": [
        "########################################################\n",
        "# PART 3 - Adaptive Filter\n",
        "########################################################\n",
        "def adaptive_filter(image_path):\n",
        "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) # Load image\n",
        "  return cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
        "            cv2.THRESH_BINARY,15,20)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdh8BsTwWPAO"
      },
      "source": [
        "## Creating images using algorithmic methods\n",
        "\n",
        "Written by Kevin Zhang"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CjFQxmSUSUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd98b462-6a87-44dd-cadb-861c332fa935"
      },
      "source": [
        "if not os.path.exists(train_path+\"/median_filtered_cropped\"):\n",
        "  os.mkdir(train_path+\"/median_filtered_cropped\")\n",
        "if not os.path.exists(train_path+\"/edge_dilation_erosion_cropped\"):\n",
        "  os.mkdir(train_path+\"/edge_dilation_erosion_cropped\")\n",
        "if not os.path.exists(train_path+\"/adaptive_filtered_cropped\"):\n",
        "  os.mkdir(train_path+\"/adaptive_filtered_cropped\")\n",
        "\n",
        "idx = 0 \n",
        "for img in sorted(os.listdir(train_path_cropped)):\n",
        "  if img.endswith('.png'):\n",
        "    input_image_path = os.path.join(train_path_cropped,img)\n",
        "\n",
        "    median_filter_path = os.path.join(train_path+\"/median_filtered_cropped\",img)\n",
        "    edge_dilation_erosion_path = os.path.join(train_path+\"/edge_dilation_erosion_cropped\",img)\n",
        "    adaptive_filter_path = os.path.join(train_path+\"/adaptive_filtered_cropped\",img)\n",
        "\n",
        "    idx += 1\n",
        "    clear_output()\n",
        "    print(\"processing\", idx)\n",
        "\n",
        "    if not os.path.exists(median_filter_path):\n",
        "      cv2.imwrite(median_filter_path, median_filter(input_image_path))\n",
        "    if not os.path.exists(edge_dilation_erosion_path):\n",
        "      cv2.imwrite(edge_dilation_erosion_path, edge_dilation_erosion_filter(input_image_path))\n",
        "    if not os.path.exists(adaptive_filter_path):\n",
        "      cv2.imwrite(adaptive_filter_path, adaptive_filter(input_image_path))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing 2448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "KhmLrIffUxhn",
        "outputId": "f4efcb7e-a843-427e-a80d-19be6e857da9"
      },
      "source": [
        "# Test image\n",
        "result = edge_dilation_erosion_filter(os.path.join(train_path_cropped, \"img_0000.png\"))\n",
        "cv2_imshow(result)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7F8AB1D34850>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uezuBRZ_aaKf"
      },
      "source": [
        "# Machine Learning Methods\n",
        "Written by Jay Yoo\n",
        "\n",
        "Adjusted by Kevin Zhang 2021-04-08\n",
        "- added train-test splits, validation, plot loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmBZaIDliPay"
      },
      "source": [
        "def prepare_params(batch_size=128, num_epochs=100, learning_rate=1e-4, \\\n",
        "                   summary_epoch_interval=10, validation_split=.15, \\\n",
        "                   shuffle=True, random_seed=0, \\\n",
        "                   loss_function=nn.MSELoss(), optimizer=torch.optim.Adam, \\\n",
        "                   model_save_path=\"./models\"):\n",
        "  # optimizer is passed the function definition, not a called function \n",
        "    # (note the lack of brackets in the base argument)\n",
        "  \n",
        "  params = {\n",
        "      'batch_size' : batch_size,\n",
        "      'num_epochs' : num_epochs,\n",
        "      'learning_rate' : learning_rate,\n",
        "      'summary_epoch_interval' : summary_epoch_interval, \n",
        "      'validation_split': validation_split,\n",
        "      'shuffle': shuffle,\n",
        "      'random_seed': random_seed,\n",
        "      'loss_function' : loss_function, \n",
        "      'optimizer' : optimizer,\n",
        "      'save_model_path': model_save_path\n",
        "  }\n",
        "\n",
        "  return params"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3TLQU9wdX5x"
      },
      "source": [
        "class model_container():\n",
        "  def __init__(self, dataset, model, params):\n",
        "    # Model does not have to be transferred to cuda gpu\n",
        "\n",
        "    self.batch_size = params['batch_size']\n",
        "    self.num_epochs = params['num_epochs']\n",
        "    self.learning_rate = params['learning_rate']\n",
        "    self.summary_epoch_interval = params['summary_epoch_interval']\n",
        "    self.val_split = params['validation_split']\n",
        "    self.shuffle = params['shuffle']\n",
        "    self.random_seed = params['random_seed']\n",
        "    self.loss_function = params['loss_function']\n",
        "    self.optimizer = params['optimizer']\n",
        "    self.save_model_path = params['save_model_path']\n",
        "    self.model = model.cuda()\n",
        "\n",
        "    if not os.path.exists(self.save_model_path):\n",
        "      os.mkdir(self.save_model_path)\n",
        "\n",
        "    # code for train-test split on custom dataset\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(self.val_split * dataset_size))\n",
        "    if self.shuffle:\n",
        "      np.random.seed(self.random_seed)\n",
        "      np.random.shuffle(indices)\n",
        "    train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    self.train_loader = DataLoader(dataset, batch_size=self.batch_size, \n",
        "                                   sampler=train_sampler, pin_memory=True, num_workers=0)\n",
        "    self.val_loader = DataLoader(dataset, batch_size=self.batch_size, \n",
        "                                        sampler=valid_sampler, pin_memory=True, num_workers=0)\n",
        "    \n",
        "\n",
        "  def train(self):\n",
        "    self.model.train()\n",
        "\n",
        "    # optimizer = torch.optim.Adam(lr=learning_rate, params=self.model.parameters())\n",
        "    optimizer = self.optimizer(lr=self.learning_rate, params=self.model.parameters())\n",
        "\n",
        "    loss_history = []\n",
        "    val_loss_history = []\n",
        "    \n",
        "    for epoch in range(1, self.num_epochs + 1):\n",
        "      cumulated_loss = 0\n",
        "\n",
        "      for model_input, labels in self.train_loader:\n",
        "        model_input = model_input.cuda()\n",
        "        labels = labels.cuda()\n",
        "        model_output = self.model(model_input)\n",
        "        loss = self.loss_function(model_output, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cumulated_loss += loss.item()\n",
        "      \n",
        "      loss_history.append(cumulated_loss)\n",
        "      \n",
        "      self.model.eval()\n",
        "      cumulated_val_loss = 0\n",
        "      \n",
        "      for model_input, labels in self.val_loader:\n",
        "        model_input = model_input.cuda()\n",
        "        labels = labels.cuda()\n",
        "        model_output = self.model(model_input)\n",
        "        loss = self.loss_function(model_output, labels)\n",
        "        cumulated_val_loss += loss.item()\n",
        "    \n",
        "      val_loss_history.append(cumulated_val_loss)\n",
        "      \n",
        "      if cumulated_val_loss <= min(val_loss_history):\n",
        "        torch.save(self.model.state_dict(), self.save_model_path+\"/AEmodel_%d\"%epoch)\n",
        "        print(\"New model saved!\")\n",
        "\n",
        "      if not epoch % self.summary_epoch_interval:\n",
        "        print(\"Epoch %d, Total training loss %0.6f, Total validation loss %0.6f\" % (epoch, cumulated_loss, cumulated_val_loss))\n",
        "        \n",
        "    plt.figure()\n",
        "    plt.plot(loss_history, \"ro-\", label=\"Train\")\n",
        "    plt.plot(val_loss_history, \"go-\", label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "\n",
        "  def inference(self, data_to_infer, save_path=None, filenames=None):\n",
        "    # Assumed to be list of numpy\n",
        "    # Add option to automatically save to a directory if memory is a concern\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    if save_path is None:\n",
        "      outputs = []\n",
        "      for data in data_to_infer:\n",
        "        model_input = torch.from_numpy(data)\n",
        "        model_input = torch.unsqueeze(model_input, dim=0)\n",
        "        model_input = torch.unsqueeze(model_input, dim=0)\n",
        "        model_input = model_input.type(torch.FloatTensor)\n",
        "        model_input = model_input.cuda()\n",
        "        outputs.append(self.model(model_input).cpu().detach().numpy() * 255)\n",
        "\n",
        "      return outputs\n",
        "    else:\n",
        "      if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "      if filenames is None:\n",
        "        filenames = [str(idx)  + '.png' for idx in range(len(data_to_infer))]\n",
        "      for idx, data in enumerate(data_to_infer):\n",
        "        model_input = torch.from_numpy(data)\n",
        "        model_input = torch.unsqueeze(model_input, dim=0)\n",
        "        model_input = torch.unsqueeze(model_input, dim=0)\n",
        "        model_input = model_input.type(torch.FloatTensor)\n",
        "        model_input = model_input.cuda()\n",
        "        model_output = self.model(model_input).cpu().detach().numpy() * 255\n",
        "\n",
        "        cv2.imwrite(os.path.join(save_path, filenames[idx]), model_output)\n",
        "\n",
        "  def delete_data(self):\n",
        "    # For memory optimizations\n",
        "    del self.dataloader\n",
        "  \n",
        "  def get_model():\n",
        "    self.model.eval()\n",
        "    return self.model"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3KVHj9maO0V"
      },
      "source": [
        "##  Autoencoder model\n",
        "Written by Jay Yoo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iosiXxWcYHV6"
      },
      "source": [
        "class autoencoder_dataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        super().__init__()\n",
        "        # data and labels are both (dataset size, m, n) where images are m x n\n",
        "\n",
        "        self.num_data = len(data)\n",
        "        self.train_data = torch.unsqueeze(torch.from_numpy(data), dim=1)\n",
        "        self.train_data = self.train_data.type(torch.FloatTensor)\n",
        "        self.labels = torch.from_numpy(labels)\n",
        "        self.labels = self.labels.type(torch.FloatTensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.train_data[idx], self.labels[idx]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgkdASRxXkAw"
      },
      "source": [
        "def autoencoder_layer(in_channels, out_channels, kernel_size=3, stride=1, \\\n",
        "                  padding=1, bias=True, batchnorm=True, activation='relu', \\\n",
        "                  upsample=None):\n",
        "    # Convolution layer that maintains shape \n",
        "        # with optional activation layer and batchnorm\n",
        "    # Use stride = 1, kernel = 3, padding = 1 for convenience\n",
        "    # Activation argument is one of 'relu', 'sigmoid', 'leaky_relu', or 'none\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    # Upsampling\n",
        "    if upsample is not None:\n",
        "        layers.append(nn.Upsample(scale_factor=upsample))\n",
        "\n",
        "    # Adding convolutional layer\n",
        "    layers.append(nn.Conv2d(in_channels=in_channels, \\\n",
        "                            out_channels=out_channels, \\\n",
        "                            kernel_size=kernel_size, \\\n",
        "                            stride=stride, \\\n",
        "                            padding=padding, \\\n",
        "                            bias=bias))\n",
        "\n",
        "    # Adding batchnorm\n",
        "    if batchnorm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    # Adding activation\n",
        "    if activation == 'relu':\n",
        "        layers.append(nn.ReLU())\n",
        "    elif activation == 'sigmoid':\n",
        "        layers.append(nn.Sigmoid())\n",
        "    elif activation == 'leaky_relu':\n",
        "        layers.append(nn.LeakyReLU())\n",
        "    elif activation == 'none':\n",
        "        pass\n",
        "    else:\n",
        "        assert False, \"Invalid activation function.\"\n",
        "        \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class autoencoder_architecture(nn.Module):\n",
        "    def __init__(self, num_hidden_channels=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net = []\n",
        "\n",
        "        self.net.append(autoencoder_layer(in_channels=1, \\\n",
        "                                      out_channels=num_hidden_channels, \\\n",
        "                                      activation='leaky_relu', \\\n",
        "                                      batchnorm=True, \\\n",
        "                                      upsample=None))\n",
        "        self.net.append(autoencoder_layer(in_channels=num_hidden_channels, \\\n",
        "                                      out_channels=num_hidden_channels, \\\n",
        "                                      activation='leaky_relu', \\\n",
        "                                      batchnorm=False, \\\n",
        "                                      upsample=None))\n",
        "        self.net.append(nn.MaxPool2d(2)) # Pool to half of shape\n",
        "        self.net.append(autoencoder_layer(in_channels=num_hidden_channels, \\\n",
        "                                      out_channels=num_hidden_channels, \\\n",
        "                                      activation='leaky_relu', \\\n",
        "                                      batchnorm=True, \\\n",
        "                                      upsample=None))\n",
        "        self.net.append(autoencoder_layer(in_channels=num_hidden_channels, \\\n",
        "                                      out_channels=num_hidden_channels, \\\n",
        "                                      activation='leaky_relu', \\\n",
        "                                      batchnorm=False, \\\n",
        "                                      upsample=2))\n",
        "        self.net.append(autoencoder_layer(in_channels=num_hidden_channels, \\\n",
        "                                      out_channels=1, \\\n",
        "                                      activation='sigmoid', \\\n",
        "                                      batchnorm=False, \\\n",
        "                                      upsample=None))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, concatenated_inputs):\n",
        "        output = self.net(concatenated_inputs)\n",
        "        return output.squeeze()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftcCNfmalbX8"
      },
      "source": [
        "def get_images_from_dir(data_path, track_progress=True, stop_idx=None, \\\n",
        "                        image_shape=None, cropped=True):\n",
        "\n",
        "  dir_paths = os.listdir(data_path)\n",
        "  num_paths = len(dir_paths)\n",
        "\n",
        "  if stop_idx is None:\n",
        "    stop_idx = num_paths\n",
        "\n",
        "  if track_progress == True:\n",
        "    def show_progress(idx):\n",
        "      clear_output()\n",
        "      print(idx, '/', stop_idx)\n",
        "  else:\n",
        "    def show_progress(idx):\n",
        "      pass\n",
        "\n",
        "  filenames = []\n",
        "  num_pngs = 0\n",
        "  image_shape = [np.infty, np.infty]\n",
        "  for idx, img in enumerate(sorted(dir_paths)):\n",
        "    if num_pngs == stop_idx:\n",
        "      break\n",
        "      \n",
        "    if img.endswith('.png'):\n",
        "      num_pngs += 1\n",
        "\n",
        "  image_shape = [num_pngs] + [256, 256]\n",
        "  images = np.zeros(image_shape)\n",
        "    \n",
        "    \n",
        "  if not cropped:\n",
        "    png_idx = 0\n",
        "    for idx, img in enumerate(sorted(dir_paths)):\n",
        "      if png_idx == stop_idx:\n",
        "        break\n",
        "\n",
        "      show_progress(idx)\n",
        "      if img.endswith('.png'):\n",
        "        filenames.append(img)\n",
        "        image_path = os.path.join(data_path,img)\n",
        "        raw_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        images[png_idx] = raw_image[0:256, 0:256] / 255\n",
        "        png_idx += 1\n",
        "  else:\n",
        "    png_idx = 0\n",
        "    for idx, img in enumerate(sorted(dir_paths)):\n",
        "      if png_idx == stop_idx:\n",
        "        break\n",
        "\n",
        "      show_progress(png_idx)\n",
        "      if img.endswith('.png'):\n",
        "        filenames.append(img)\n",
        "        image_path = os.path.join(data_path,img)\n",
        "        raw_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        images[png_idx] = raw_image/255\n",
        "        png_idx += 1\n",
        "\n",
        "  return images, image_shape, filenames"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHo04LxhmI7G",
        "outputId": "4cf8d666-0e1e-4357-c644-a1a6f40a9d34"
      },
      "source": [
        "autoencoder_train_data, shape, train_filenames = get_images_from_dir(train_path_cropped)\n",
        "autoencoder_labels, shape, _ = get_images_from_dir(train_label_path_cropped)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2447 / 2448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQo3Xyq9jasN"
      },
      "source": [
        "dataset = autoencoder_dataset(autoencoder_train_data, autoencoder_labels)\n",
        "autoencoder_model = autoencoder_architecture(num_hidden_channels=64)\n",
        "autoencoder_params = prepare_params(batch_size=32, summary_epoch_interval=1, num_epochs=100, learning_rate=1e-4)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT5jMCW3jwRu"
      },
      "source": [
        "autoencoder = model_container(dataset, autoencoder_model, autoencoder_params)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2gqFlRfkX-u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df7550ad-4eb8-4818-8184-70567aa86ad0"
      },
      "source": [
        "autoencoder.train()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1, 256, 256])) that is different to the input size (torch.Size([256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "New model saved!\n",
            "Epoch 1, Total training loss 3.855344, Total validation loss 0.197453\n",
            "New model saved!\n",
            "Epoch 2, Total training loss 0.540544, Total validation loss 0.068965\n",
            "New model saved!\n",
            "Epoch 3, Total training loss 0.308325, Total validation loss 0.049062\n",
            "New model saved!\n",
            "Epoch 4, Total training loss 0.236588, Total validation loss 0.044046\n",
            "New model saved!\n",
            "Epoch 5, Total training loss 0.200335, Total validation loss 0.035327\n",
            "New model saved!\n",
            "Epoch 6, Total training loss 0.175782, Total validation loss 0.031652\n",
            "New model saved!\n",
            "Epoch 7, Total training loss 0.159258, Total validation loss 0.029086\n",
            "New model saved!\n",
            "Epoch 8, Total training loss 0.148118, Total validation loss 0.026903\n",
            "New model saved!\n",
            "Epoch 9, Total training loss 0.139351, Total validation loss 0.026215\n",
            "New model saved!\n",
            "Epoch 10, Total training loss 0.131151, Total validation loss 0.024236\n",
            "New model saved!\n",
            "Epoch 11, Total training loss 0.124214, Total validation loss 0.023629\n",
            "New model saved!\n",
            "Epoch 12, Total training loss 0.120098, Total validation loss 0.021975\n",
            "New model saved!\n",
            "Epoch 13, Total training loss 0.114311, Total validation loss 0.021290\n",
            "New model saved!\n",
            "Epoch 14, Total training loss 0.109875, Total validation loss 0.020522\n",
            "Epoch 15, Total training loss 0.109003, Total validation loss 0.026929\n",
            "New model saved!\n",
            "Epoch 16, Total training loss 0.107160, Total validation loss 0.019778\n",
            "New model saved!\n",
            "Epoch 17, Total training loss 0.099981, Total validation loss 0.018741\n",
            "New model saved!\n",
            "Epoch 18, Total training loss 0.097169, Total validation loss 0.018440\n",
            "New model saved!\n",
            "Epoch 19, Total training loss 0.094682, Total validation loss 0.017541\n",
            "Epoch 20, Total training loss 0.094493, Total validation loss 0.019679\n",
            "New model saved!\n",
            "Epoch 21, Total training loss 0.091355, Total validation loss 0.016845\n",
            "New model saved!\n",
            "Epoch 22, Total training loss 0.088531, Total validation loss 0.016799\n",
            "New model saved!\n",
            "Epoch 23, Total training loss 0.088379, Total validation loss 0.016266\n",
            "Epoch 24, Total training loss 0.087481, Total validation loss 0.016488\n",
            "Epoch 25, Total training loss 0.085788, Total validation loss 0.034938\n",
            "New model saved!\n",
            "Epoch 26, Total training loss 0.093334, Total validation loss 0.015441\n",
            "New model saved!\n",
            "Epoch 27, Total training loss 0.080541, Total validation loss 0.015227\n",
            "Epoch 28, Total training loss 0.079246, Total validation loss 0.015580\n",
            "Epoch 29, Total training loss 0.079094, Total validation loss 0.017923\n",
            "New model saved!\n",
            "Epoch 30, Total training loss 0.079128, Total validation loss 0.014525\n",
            "Epoch 31, Total training loss 0.076387, Total validation loss 0.014577\n",
            "Epoch 32, Total training loss 0.075408, Total validation loss 0.015454\n",
            "Epoch 33, Total training loss 0.074937, Total validation loss 0.017343\n",
            "Epoch 34, Total training loss 0.073756, Total validation loss 0.014809\n",
            "New model saved!\n",
            "Epoch 35, Total training loss 0.072000, Total validation loss 0.013448\n",
            "Epoch 36, Total training loss 0.070787, Total validation loss 0.013521\n",
            "Epoch 37, Total training loss 0.071075, Total validation loss 0.013634\n",
            "New model saved!\n",
            "Epoch 38, Total training loss 0.070415, Total validation loss 0.012960\n",
            "Epoch 39, Total training loss 0.068519, Total validation loss 0.013452\n",
            "Epoch 40, Total training loss 0.067547, Total validation loss 0.013256\n",
            "Epoch 41, Total training loss 0.067414, Total validation loss 0.013493\n",
            "New model saved!\n",
            "Epoch 42, Total training loss 0.066738, Total validation loss 0.012536\n",
            "New model saved!\n",
            "Epoch 43, Total training loss 0.065174, Total validation loss 0.012136\n",
            "Epoch 44, Total training loss 0.065106, Total validation loss 0.013433\n",
            "Epoch 45, Total training loss 0.064372, Total validation loss 0.012637\n",
            "New model saved!\n",
            "Epoch 46, Total training loss 0.063163, Total validation loss 0.011776\n",
            "New model saved!\n",
            "Epoch 47, Total training loss 0.062169, Total validation loss 0.011720\n",
            "Epoch 48, Total training loss 0.062108, Total validation loss 0.011728\n",
            "New model saved!\n",
            "Epoch 49, Total training loss 0.061362, Total validation loss 0.011394\n",
            "New model saved!\n",
            "Epoch 50, Total training loss 0.060473, Total validation loss 0.011320\n",
            "Epoch 51, Total training loss 0.059604, Total validation loss 0.011786\n",
            "New model saved!\n",
            "Epoch 52, Total training loss 0.060040, Total validation loss 0.011109\n",
            "Epoch 53, Total training loss 0.058809, Total validation loss 0.011791\n",
            "Epoch 54, Total training loss 0.058864, Total validation loss 0.011221\n",
            "New model saved!\n",
            "Epoch 55, Total training loss 0.057104, Total validation loss 0.010767\n",
            "Epoch 56, Total training loss 0.057183, Total validation loss 0.011193\n",
            "New model saved!\n",
            "Epoch 57, Total training loss 0.057213, Total validation loss 0.010576\n",
            "Epoch 58, Total training loss 0.055922, Total validation loss 0.010697\n",
            "Epoch 59, Total training loss 0.055819, Total validation loss 0.014916\n",
            "Epoch 60, Total training loss 0.056664, Total validation loss 0.010670\n",
            "New model saved!\n",
            "Epoch 61, Total training loss 0.054476, Total validation loss 0.010164\n",
            "New model saved!\n",
            "Epoch 62, Total training loss 0.053390, Total validation loss 0.010155\n",
            "Epoch 63, Total training loss 0.053044, Total validation loss 0.012208\n",
            "New model saved!\n",
            "Epoch 64, Total training loss 0.053635, Total validation loss 0.009790\n",
            "Epoch 65, Total training loss 0.052776, Total validation loss 0.010790\n",
            "New model saved!\n",
            "Epoch 66, Total training loss 0.053154, Total validation loss 0.009673\n",
            "Epoch 67, Total training loss 0.051397, Total validation loss 0.011379\n",
            "Epoch 68, Total training loss 0.051873, Total validation loss 0.010175\n",
            "Epoch 69, Total training loss 0.051062, Total validation loss 0.009814\n",
            "Epoch 70, Total training loss 0.050986, Total validation loss 0.010357\n",
            "New model saved!\n",
            "Epoch 71, Total training loss 0.050992, Total validation loss 0.009465\n",
            "Epoch 72, Total training loss 0.049177, Total validation loss 0.009974\n",
            "Epoch 73, Total training loss 0.050122, Total validation loss 0.009515\n",
            "Epoch 74, Total training loss 0.049574, Total validation loss 0.010216\n",
            "New model saved!\n",
            "Epoch 75, Total training loss 0.049225, Total validation loss 0.009331\n",
            "Epoch 76, Total training loss 0.049746, Total validation loss 0.010265\n",
            "Epoch 77, Total training loss 0.048564, Total validation loss 0.009907\n",
            "New model saved!\n",
            "Epoch 78, Total training loss 0.047330, Total validation loss 0.008903\n",
            "New model saved!\n",
            "Epoch 79, Total training loss 0.047207, Total validation loss 0.008745\n",
            "Epoch 80, Total training loss 0.046560, Total validation loss 0.009588\n",
            "Epoch 81, Total training loss 0.046614, Total validation loss 0.008760\n",
            "Epoch 82, Total training loss 0.046108, Total validation loss 0.009456\n",
            "New model saved!\n",
            "Epoch 83, Total training loss 0.045609, Total validation loss 0.008618\n",
            "Epoch 84, Total training loss 0.046065, Total validation loss 0.010375\n",
            "New model saved!\n",
            "Epoch 85, Total training loss 0.045761, Total validation loss 0.008444\n",
            "Epoch 86, Total training loss 0.044649, Total validation loss 0.008549\n",
            "New model saved!\n",
            "Epoch 87, Total training loss 0.044081, Total validation loss 0.008259\n",
            "Epoch 88, Total training loss 0.043744, Total validation loss 0.008313\n",
            "Epoch 89, Total training loss 0.043689, Total validation loss 0.008370\n",
            "Epoch 90, Total training loss 0.044045, Total validation loss 0.008783\n",
            "Epoch 91, Total training loss 0.043487, Total validation loss 0.010588\n",
            "Epoch 92, Total training loss 0.043287, Total validation loss 0.008410\n",
            "New model saved!\n",
            "Epoch 93, Total training loss 0.042779, Total validation loss 0.007991\n",
            "New model saved!\n",
            "Epoch 94, Total training loss 0.042520, Total validation loss 0.007927\n",
            "Epoch 95, Total training loss 0.041962, Total validation loss 0.009053\n",
            "Epoch 96, Total training loss 0.042532, Total validation loss 0.009262\n",
            "New model saved!\n",
            "Epoch 97, Total training loss 0.041546, Total validation loss 0.007921\n",
            "Epoch 98, Total training loss 0.041010, Total validation loss 0.008271\n",
            "Epoch 99, Total training loss 0.040462, Total validation loss 0.008338\n",
            "New model saved!\n",
            "Epoch 100, Total training loss 0.040021, Total validation loss 0.007661\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQTXNqPNXWIB"
      },
      "source": [
        "# Set MODEL PATH for loading model\n",
        "PATH = \"/content/drive/MyDrive/University Academics/Year 4 EngSci/CSC413 Project/models/AEmodel_100\"\n",
        "\n",
        "model = autoencoder_architecture(num_hidden_channels=64)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\n",
        "autoencoder = model_container(dataset, model, autoencoder_params)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6I1jZ10RgiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c28769cd-436d-412f-c303-6b2bb824f865"
      },
      "source": [
        "autoencoder_test_data, shapes, test_filenames = get_images_from_dir(test_path, cropped=False)\n",
        "test_outputs = autoencoder.inference(autoencoder_test_data, save_path=test_output_path, filenames=test_filenames)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "432 / 433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXDw0e0hfi4P"
      },
      "source": [
        "cv2_imshow(test_outputs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh4cmVmJXaif"
      },
      "source": [
        "# Generating output for saving\n",
        "outputs = autoencoder.inference(autoencoder_train_data, save_path=output_path, filenames=train_filenames)"
      ],
      "execution_count": 92,
      "outputs": []
    }
  ]
}